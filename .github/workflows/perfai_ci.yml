name: PerfAI Performance Regression Detection

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main ]
  schedule:
    # Run daily at 2 AM UTC
    - cron: '0 2 * * *'
  workflow_dispatch:
    inputs:
      run_extended:
        description: 'Run extended benchmark suite'
        required: false
        default: 'false'
        type: boolean

env:
  CUDA_VERSION: '11.8'
  PYTHON_VERSION: '3.9'

jobs:
  build-cuda:
    runs-on: ubuntu-latest
    container:
      image: nvidia/cuda:11.8-devel-ubuntu20.04
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v3
    
    - name: Install dependencies
      run: |
        apt-get update
        apt-get install -y build-essential cmake git
        apt-get install -y python3 python3-pip
    
    - name: Build CUDA components
      run: |
        make clean
        make all
        make test
    
    - name: Upload CUDA artifacts
      uses: actions/upload-artifact@v3
      with:
        name: cuda-benchmark
        path: bin/cuda_benchmark
        retention-days: 30

  setup-python:
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v3
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        cache: 'pip'
    
    - name: Install Python dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
    
    - name: Run Python tests
      run: |
        python -m pytest tests/ -v --cov=src/
    
    - name: Lint Python code
      run: |
        flake8 src/ --max-line-length=100
        black --check src/
        isort --check-only src/

  performance-regression-test:
    runs-on: self-hosted  # Requires GPU runner
    needs: [build-cuda, setup-python]
    if: github.event_name != 'pull_request' || contains(github.event.pull_request.labels.*.name, 'performance-test')
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v3
    
    - name: Download CUDA artifacts
      uses: actions/download-artifact@v3
      with:
        name: cuda-benchmark
        path: bin/
    
    - name: Make executable
      run: chmod +x bin/cuda_benchmark
    
    - name: Set up Python environment
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
    
    - name: Check GPU availability
      run: |
        nvidia-smi
        bin/cuda_benchmark --test-mode --verbose
    
    - name: Run performance baseline check
      id: baseline_check
      run: |
        # Check if baseline exists
        if [ -f data/baselines/latest_baseline.json ]; then
          echo "baseline_exists=true" >> $GITHUB_OUTPUT
        else
          echo "baseline_exists=false" >> $GITHUB_OUTPUT
        fi
    
    - name: Create initial baseline (if needed)
      if: steps.baseline_check.outputs.baseline_exists == 'false'
      run: |
        # Run benchmark to create baseline
        python src/pipeline/perfai_pipeline.py --benchmark-only
        python src/pipeline/data_pipeline.py baseline --baseline-name initial_baseline
        echo "Created initial baseline"
    
    - name: Run performance regression detection
      id: regression_test
      run: |
        # Run full pipeline
        python src/pipeline/perfai_pipeline.py --full-pipeline \
          --output-dir "results_${{ github.run_number }}" \
          --config config.json
        
        # Capture exit code for later use
        echo "exit_code=$?" >> $GITHUB_OUTPUT
    
    - name: Upload performance results
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: performance-results-${{ github.run_number }}
        path: results_${{ github.run_number }}/
        retention-days: 90
    
    - name: Comment PR with results
      if: github.event_name == 'pull_request'
      uses: actions/github-script@v6
      with:
        script: |
          const fs = require('fs');
          const path = 'results_${{ github.run_number }}/performance_analysis_report.txt';
          
          if (fs.existsSync(path)) {
            const report = fs.readFileSync(path, 'utf8');
            const comment = `## PerfAI Performance Analysis Results\n\n\`\`\`\n${report}\n\`\`\``;
            
            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: comment
            });
          }
    
    - name: Fail on high severity regression
      if: steps.regression_test.outputs.exit_code == '2'
      run: |
        echo "❌ High severity performance regression detected!"
        echo "Please review the performance analysis report."
        exit 1
    
    - name: Warn on medium severity regression
      if: steps.regression_test.outputs.exit_code == '1'
      run: |
        echo "⚠️ Medium severity performance regression detected!"
        echo "Consider reviewing the performance analysis report."

  extended-benchmark:
    runs-on: self-hosted
    needs: [build-cuda, setup-python]
    if: github.event.inputs.run_extended == 'true' || github.event_name == 'schedule'
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v3
    
    - name: Download CUDA artifacts
      uses: actions/download-artifact@v3
      with:
        name: cuda-benchmark
        path: bin/
    
    - name: Make executable
      run: chmod +x bin/cuda_benchmark
    
    - name: Set up Python environment
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
    
    - name: Run extended benchmark suite
      run: |
        # Create extended config
        cat > extended_config.json << EOF
        {
          "benchmark_config": {
            "matrix_sizes": [256, 512, 1024, 2048, 4096, 8192],
            "iterations": 20,
            "workloads": ["gemm", "conv", "bandwidth"]
          },
          "confidence_threshold": 0.99,
          "contamination_rate": 0.05
        }
        EOF
        
        # Run extended pipeline
        python src/pipeline/perfai_pipeline.py --full-pipeline \
          --config extended_config.json \
          --output-dir "extended_results_${{ github.run_number }}"
    
    - name: Upload extended results
      uses: actions/upload-artifact@v3
      with:
        name: extended-performance-results-${{ github.run_number }}
        path: extended_results_${{ github.run_number }}/
        retention-days: 180

  deploy-baseline:
    runs-on: ubuntu-latest
    needs: [performance-regression-test]
    if: github.ref == 'refs/heads/main' && github.event_name == 'push'
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v3
    
    - name: Download performance results
      uses: actions/download-artifact@v3
      with:
        name: performance-results-${{ github.run_number }}
        path: results/
    
    - name: Update baseline (if stable)
      run: |
        # Check if results are stable (no high severity issues)
        if [ ! -f results/high_severity_detected ]; then
          echo "Results stable, updating baseline..."
          # Here you would implement baseline update logic
          # For example, commit new baseline to repository
        else
          echo "High severity issues detected, not updating baseline"
        fi

  notify:
    runs-on: ubuntu-latest
    needs: [performance-regression-test, extended-benchmark]
    if: always() && (failure() || cancelled())
    
    steps:
    - name: Notify on failure
      uses: actions/github-script@v6
      with:
        script: |
          const title = "PerfAI Pipeline Failure";
          const body = `
          The PerfAI performance regression detection pipeline has failed.
          
          **Run ID**: ${{ github.run_number }}
          **Workflow**: ${{ github.workflow }}
          **Trigger**: ${{ github.event_name }}
          **Ref**: ${{ github.ref }}
          
          Please check the workflow logs for details.
          `;
          
          // In a real setup, you would send notifications to Slack, email, etc.
          console.log(title);
          console.log(body);
